---
description: >-
  investigate into the existing realization of lock-free queue,compare them and
  pick up some common ideas
---

# ğŸ™ƒ å·²æœ‰æ— é”é˜Ÿåˆ—è°ƒç ”

## å‚è€ƒä»£ç åº“

* moodycamel::ReaderWriterQueue(spsc)
* moodycamel::ConcurrentQueue(mpmc)
* atomic\_queue::AtomicQueue(Îµ|2|B|B2)
* boost::lockfree::spsc\_queue
* boost::lockfree::queue

## ç®€å•æ— é”é˜Ÿåˆ—

* æ— é˜Ÿåˆ—é•¿åº¦é™åˆ¶ã€æ— ç¼“å†²åŒºçš„ï¼šç”¨é“¾è¡¨å®ç°ï¼Œå¯¹é“¾è¡¨å¤´ã€å°¾ä»¥åŠnextæŒ‡é’ˆè¿›è¡ŒatomicåŒ…è£…
* æœ‰é˜Ÿåˆ—é•¿åº¦é™åˆ¶ã€æœ‰ç¼“å†²åŒºçš„ï¼šç”¨T\[]çš„bufferæ­é…å¤´å°¾çš„indexå®ç°ï¼Œå…¶ä¸­å¤´å°¾ç”¨atomicåŒ…è£…
* spscå› å¯èƒ½æŒ‡ä»¤é‡æ’ä¸‹é¢ç†è®ºä¸å¯è¡Œï¼Œå¿…é¡»å€ŸåŠ©atomic

<figure><img src="../.gitbook/assets/image (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

* ç”¨ä½åŸŸæ¥å­˜å‚¨å¼•ç”¨è®¡æ•°å¹¶èƒ½æ”¯æŒæ— é”atomicæ“ä½œï¼Œå®ç°å¯¹nodeç»“ç‚¹çš„æ­£ç¡®é‡Šæ”¾ï¼Ÿ

## moodycamel::readerwriterqueue

### Features

* Blazing fast
* Compatible with C++11 (supports moving objects instead of making copies)
* Fully generic (templated container of any type) -- just like `std::queue`, you never need to allocate memory for elements yourself (which saves you the hassle of writing a lock-free memory manager to hold the elements you're queueing)
* Allocates memory up front, in contiguous blocks
* Provides a `try_enqueue` method which is guaranteed never to allocate memory (the queue starts with an initial capacity)
* Also provides an `enqueue` method which can dynamically grow the size of the queue as needed
* Also provides `try_emplace`/`emplace` convenience methods
* Has a blocking version with `wait_dequeue`
* Completely "wait-free" (no compare-and-swap loop). Enqueue and dequeue are always O(1) (not counting memory allocation)
* On x86, the memory barriers compile down to no-ops, meaning enqueue and dequeue are just a simple series of loads and stores (and branches)

### Findings

* ä»…åœ¨x86\64ä¸Šæµ‹è¯•è¿‡ï¼Œä½†åªè¦atomicops.hçš„å®ç°æ­£ç¡®ï¼Œå¹¶ä¸”å¯¹é½çš„æ•´æ•°/æŒ‡é’ˆè®¿é—®å…·æœ‰è‡ªç„¶çš„åŸå­æ€§ï¼Œå¯¹äºä»»ä½•æ¶æ„è¿™ä¸ªå®ç°å°±åº”è¯¥æ˜¯å¯¹çš„ã€‚
* ä»…å¯¹å›ºå®šä¸€ä¸ªç”Ÿäº§è€…çº¿ç¨‹+å›ºå®šä¸€ä¸ªæ¶ˆè´¹è€…çº¿ç¨‹çº¿ç¨‹å®‰å…¨ï¼Œå¦åˆ™ä¸èƒ½ä¿è¯
* å—æŸè®ºæ–‡å¯å‘ï¼Œå¯¹ç¼“å­˜å‹å¥½æ€§è¿›è¡Œäº†ä¼˜åŒ–ã€‚
* ä½å±‚æ¬¡çš„é˜Ÿåˆ—æ˜¯å¾ªç¯æ•°ç»„ï¼ˆç¼“å†²åŒºä¸ºblock)ï¼Œfront,tail are indices,when tail == front the queue is empty and can't be full,åˆ†åˆ«å¯¹åº”å‡ºé˜Ÿå…ƒç´ ç´¢å¼•å’Œæ’å…¥ä½ç½®ç´¢å¼•ï¼Œæ°å¥½æœ‰ä¸€ä¸ªå…ƒç´ è¢«æµªè´¹ï¼›é«˜å±‚æ¬¡æ˜¯blockç»„æˆçš„é“¾è¡¨é˜Ÿåˆ—ï¼Œfrontæ˜¯å‡ºé˜Ÿå—ï¼ˆéç©ºçš„è¯ï¼‰ï¼Œbackæ˜¯å…¥é˜Ÿå—ï¼ˆéæ»¡çš„è¯ï¼‰ã€‚pæ‹¥æœ‰tailï¼Œcæ‹¥æœ‰frontï¼ˆè¯»å…±äº«ï¼Œå†™åªå†™æ‰€æ‹¥æœ‰çš„ï¼‰
* å¦‚æœå…¥é˜Ÿå—ç©ºé—´ä¸å¤Ÿï¼Œåˆ™åˆ†é…æ–°çš„å—ã€‚å—æ°¸ä¸åˆ é™¤ã€‚
* åˆ©ç”¨å†…éƒ¨ç»“æ„ä½“blockæ¥åˆ†é…å—ï¼Œé¿å…å‡å…±äº«ï¼Ÿè¿™é‡Œçš„blockå°±æ˜¯ä½å±‚æ¬¡é˜Ÿåˆ—ã€‚
  * åˆ©ç”¨localTailæ¥åŠ é€Ÿtailå¯¹consumerçš„è®¿é—®ï¼ŒlocalFrontåŒç†
  * frontå’Œtailéƒ½æ˜¯weak\_atomic\<size\_t>ç´¢å¼•ï¼Œè€Œnextä¸ºweak\_atmoic\<Block\*>æŒ‡é’ˆç”¨äºæŒ‡å‘ä¸‹ä¸€ä¸ªblockã€‚
  * **ç”¨cachelineFillerå­—ç¬¦æ•°ç»„å¡«å……Blockç»“æ„ä½“ï¼Œæ¥è®©åŸå­å˜é‡ä¸åœ¨åŒä¸€cachelineï¼Œé¿å…ä¼ªå…±äº«ã€‚**
  * char\*æŒ‡é’ˆæŒ‡å‘å®é™…å…ƒç´ çš„åœ°å€ï¼Œä¿è¯äº†Blockçš„sizeå¯¹ä»»ä¸€æ¨¡æ¿å‚æ•°éƒ½æ˜¯å›ºå®šçš„ï¼ˆæ§åˆ¶å˜é‡ï¼‰ã€‚
  * sizeMaskæ˜¯å®é™…èƒ½å®¹çº³å…ƒç´ ä¸ªæ•°ï¼Œæ¯”dataåˆ†é…å¤§å°å°‘1
  * rawThisæŒ‡å‘make\_blockä¸­ä¸ºå¯¹é½è€Œåˆ†é…åˆ°çš„åŸå§‹å—çš„å¤§å°ã€‚
* fence(memory\_order\_sync)ç”¨æ¥è®©æ‰€æœ‰ç›¸å…³çº¿ç¨‹åŒæ­¥ã€‚
* ReentrantGuardç”¨æ¥åœ¨debugæ¨¡å¼ä¸‹æŒ‡ç¤ºæ˜¯å¦å¤šä¸ªæ¶ˆè´¹è€…/ç”Ÿäº§è€…è¿›å…¥é˜Ÿåˆ—ã€‚
* frontå®Œå…¨ç”±æ¶ˆè´¹è€…æ›´æ–°ï¼Œæ‰€ä»¥åœ¨æ¶ˆè´¹è€…ç«¯ä¸éœ€è¦åŒæ­¥ï¼Œç”Ÿäº§è€…ç«¯éœ€è¦åŒæ­¥ã€‚tailå®Œå…¨ç”±ç”Ÿäº§è€…æ›´æ–°ï¼ŒåŒç†ã€‚æ¶ˆè´¹è€…æ‹¥æœ‰localTail,ç”Ÿäº§è€…æ‹¥æœ‰localFront.
*

### members

* largestBlockSize ç”±åˆå§‹åŒ–çš„sizeå‚æ•°å†³å®šï¼Œä¸ºsizeçš„äºŒæ¬¡å¹‚é¡¶æˆ–MAX\_BLOCK\_SIZEã€‚è‹¥äºŒæ¬¡å¹‚é¡¶<=2\*MBSï¼Œåˆ™å…ˆåˆ†é…ä¸€ä¸ªMBSçš„å—ï¼›å¦åˆ™å°†largestBlockSizeæ›´æ–°ä¸ºMAX\_BLOCK\_SIZEï¼ˆæ­¤æ—¶size>MBSï¼‰ï¼Œå¹¶åˆ†é…initialBlockCountä¸ªå—,ä¿è¯æœ‰ä¸€ä¸ªç©ºé—²å—å¤–å‰©ä¸‹çš„å—slotså¤§äºç­‰äºsizeã€‚æ„é€ ç»“æŸæ—¶ï¼Œæœ€åä¸€ä¸ªblockçš„nextæŒ‡å‘firstblock
* ç§»åŠ¨æ„é€ å‡½æ•°æ²¡æœ‰äº’æ–¥å¤„ç†ï¼Œéœ€è¦ç”¨æˆ·è‡ªå·±åŒæ­¥ï¼Œæ„é€ åotheråªå‰©ä¸‹ä¸€ä¸ªæ–°çš„å—
* ç§»åŠ¨èµ‹å€¼å‡½æ•°äº¤æ¢å„å‚æ•°ï¼Œé—´æ¥äº¤æ¢å—ï¼Œéœ€æ‰‹åŠ¨åŒæ­¥
* ææ„æ—¶ é¦–å…ˆä¸€ä¸ªæœ€å¼ºçš„å†…å­˜åŒæ­¥æ¥è·å–æœ€æ–°å‚æ•°ï¼ˆå³Frontblock,tailblockï¼‰ï¼Œéšåä¾æ¬¡å¯¹blockä¸­å…ƒç´ è¿›è¡Œææ„ã€‚è¿™é‡Œå¯¹äºsizeMask(ä½ä½å…¨ä¸€æ•°ï¼‰ç”¨&æ¯”%å¿«ã€‚
* try\_xxxä¸xxxåªæ˜¯è°ƒç”¨inner\_xxxè¿™ä¸ªæ¨¡æ¿å‡½æ•°ï¼Œæ ¹æ®enumæ¨¡æ¿å˜é‡å†³å®šæ˜¯å¦è¦åˆ†é…ç©ºé—´ã€‚å‡å°‘ä»£ç é‡å¤ã€‚
*

### static member

* make\_blockæ˜¯åˆ†é…å¤§å°ä¸ºcapacityçš„blockå—(å®é™…å¯ç”¨å¤§å°ä¸ºcapacity-1ï¼Œå³sizeMask)ï¼Œåˆ†é…æ ¼å±€ä¸ºï¼š|å¯èƒ½çš„blockå¯¹é½å¡«å……|block|å¯èƒ½çš„blockå¯¹é½å¡«å……|å¯èƒ½çš„Tå…ƒç´ å¯¹é½å¡«å……|capacityä¸ªTå…ƒç´ å—ï¼ˆç›¸é‚»ä¸¤ä¸ªä¹‹é—´æœ‰é»˜è®¤å¯¹é½å¡«å……ï¼‰|å¯èƒ½çš„Tå…ƒç´ å¯¹é½å¡«å……ã€‚    æ³¨æ„åˆ°ç”±äºéœ€è¦æ‰‹åŠ¨placement newå°†Blockå’Œelementsåˆ†é…åˆ°é‚»è¿‘çš„å†…å­˜åœ°å€ï¼Œæ‰€ä»¥éœ€è¦æ‰‹åŠ¨è®¡ç®—åˆé€‚å¯¹é½åçš„å†…å­˜åˆ†é…åœ°å€ã€‚            æ‰‹åŠ¨åˆ†é…åœ°å€çš„å¥½å¤„ï¼šä»…è°ƒç”¨ä¸€æ¬¡å †åˆ†é…ï¼ˆmallocï¼‰ï¼Œä¸”ç©ºé—´å±€éƒ¨æ€§æ›´å¥½ã€‚
  * æœ€ç»ˆè¿”å›æ„é€ å¥½çš„Blockçš„åœ°å€ã€‚
* ceilToPow2æ˜¯æ‰¾åˆ°å¤§äºç­‰äºxçš„æœ€å°äºŒæ¬¡å¹‚ï¼Œåˆ©ç”¨ä½è¿ç®—å†…è”åŠ é€Ÿã€‚
* align\_foræ˜¯ä¸€ä¸ªæ‰¾åˆ°Uç±»å‹å¯¹è±¡åœ¨ptrå¼€å§‹çš„åœ°å€ä¸­ç¬¬ä¸€ä¸ªå¯¹é½çš„åœ°å€ï¼Œè¿ç”¨å†…è”ä¼˜åŒ–ã€‚

### atomicops.h

* memory\_orderè¿™ä¸ªæšä¸¾ç±»ç”¨æ¥æä¾›ç»Ÿä¸€çš„ä¸‹å±‚æ¥å£
* åªè€ƒè™‘ç¼–è¯‘ç¯å¢ƒä¸‹ç”¨atomicåº“å®ç°çš„atomicops.hå†…å®¹ï¼Œå› ä¸ºVSæ˜¾ç¤ºä¼šè¿™ä¹ˆç¼–è¯‘
* weak\_atomicç±»å°è£…äº†atomicçš„relaxedå­˜å–ä¸åŠ è½½ï¼Œfetch\_add\_acquireæ˜¯acquireè¯­ä¹‰ï¼Œfetch\_add\_releaseæ˜¯releaseè¯­ä¹‰ã€‚



## moodycamel::BlockingReaderWriterQueue

* ç”¨äº†ä¸€ä¸ªLightweightSemaphoreæ¥è¿›è¡Œè‡ªæ—‹ç­‰å¾…ã€é€šçŸ¥ã€å®šæ—¶é˜»å¡ã€‚å†…éƒ¨ç”¨ReaderWriterQueueä½œä¸ºå­æ•°æ®ç»“æ„ã€‚æä¾›é˜»å¡å’Œéé˜»å¡æ¥å£ã€‚
* SPSC

## moodycamel::BlockingReaderWriterCircularBuffer

* SPSC
* å¾ªç¯ç¼“å†²åŒºï¼Œç¼“å†²åŒºå¤§å°ä¸€å®šã€‚ä¸ReaderWriterQueueä¸åŒï¼Œæ²¡æœ‰blockã€‚ç”¨LightweightSemaphoreå®ç°äº†è‡ªé€‰ç­‰å¾…ã€é€šçŸ¥ã€å®šæ—¶é˜»å¡çš„å­åŠŸèƒ½ã€‚æœ‰é˜»å¡å’Œéé˜»å¡æ¥å£ã€‚

## moodycamel::ConcurrentQueue

### Features

* Knock-your-socks-off [blazing fast performance](http://moodycamel.com/blog/2014/a-fast-general-purpose-lock-free-queue-for-c++#benchmarks).
* Single-header implementation. Just drop it in your project.
* Fully thread-safe lock-free queue. Use concurrently from any number of threads.
* C++11 implementation -- elements are moved (instead of copied) where possible.
* Templated, obviating the need to deal exclusively with pointers -- memory is managed for you.
* No artificial limitations on element types or maximum count.
* Memory can be allocated once up-front, or dynamically as needed.
* Fully portable (no assembly; all is done through standard C++11 primitives).
* Supports super-fast bulk operations.
* Includes a low-overhead blocking version (BlockingConcurrentQueue).
* Exception safe.

### Constraints

* **not linearizable** (see the next section on high-level design). The foundations of its design assume that producers are independent; if this is not the case, and your producers co-ordinate amongst themselves in some fashion, be aware that the elements won't necessarily come out of the queue in the same order they were put in _relative to the ordering formed by that co-ordination_ (but they will still come out in the order they were put in by any _individual_ producer). If this affects your use case, you may be better off with another implementation; either way, it's an important limitation to be aware of.
* **not NUMA aware**, and does a lot of memory re-use internally, meaning it probably doesn't scale particularly well on NUMA architectures; however, I don't know of any other lock-free queue that _is_ NUMA aware (except for [SALSA](http://webee.technion.ac.il/\~idish/ftp/spaa049-gidron.pdf), which is very cool, but has no publicly available implementation that I know of).
* &#x20;**not sequentially consistent**; there _is_ a happens-before relationship between when an element is put in the queue and when it comes out, but other things (such as pumping the queue until it's empty) require more thought to get right in all eventualities, because explicit memory ordering may have to be done to get the desired effect. In other words, it can sometimes be difficult to use the queue correctly. This is why it's a good idea to follow the [samples](https://github.com/cameron314/concurrentqueue/blob/master/samples.md) where possible. On the other hand, the upside of this lack of sequential consistency is better performance.

### Efforts

*

    <figure><img src="../.gitbook/assets/image (2) (1).png" alt=""><figcaption></figcaption></figure>

### Reference links

{% embed url="https://moodycamel.com/blog/2014/a-fast-general-purpose-lock-free-queue-for-c++" %}

{% embed url="https://moodycamel.com/blog/2014/detailed-design-of-a-lock-free-queue" %}

### Findings

* a set of sub-queues is used,one for each producder thread.This means that different threads ca enqueue items completely in parallel, independently of each other.
* All elements from a given producer thread will necessarily still be seen in that same order relative to each other when dequeued (since the sub-queue preserves that order), albeit with elements from other sub-queues possibly interleaved
* ç”¨cusumerTokenæ¥æä¾›å°±è¿‘åŒ¹é…çš„ä¿¡æ¯ï¼Œè€Œå¦‚æœæ²¡æœ‰è¿™ä¸ªtokenå°±æ˜¯å¹³å¸¸çš„éå†
* æ¯ä¸ªå­é˜Ÿåˆ—ç”¨çš„æ˜¯æ•°ç»„è€Œéé“¾è¡¨ï¼Œä¾ç„¶æ˜¯blockså’Œinnerindicesï¼Œç”±äºä¸Producerä¸€ä¸€å¯¹åº”ï¼Œtailæ— å†™ç«äº‰ï¼Œheadæœ‰å†™ç«äº‰ï¼Œè€Œheadæœ‰å›é€€æœºåˆ¶é˜²æ­¢å‡ºé”™ã€‚

> So, that's the high-level design. What about the core algorithm used within each sub-queue? Well, instead of being based on a linked-list of nodes (which implies constantly allocating and freeing or re-using elements, and typically relies on a compare-and-swap loop which can be slow under heavy contention), I based my queue on an array model. Instead of linking individual elements, I have a "block" of several elements. The logical head and tail indices of the queue are represented using atomically-incremented integers. Between these logical indices and the blocks lies a scheme for mapping each index to its block and sub-index within that block. An enqueue operation simply increments the tail (remember that there's only one producer thread for each sub-queue). A dequeue operation increments the head if it sees that the head is less than the tail, and then it checks to see if it accidentally incremented the head past the tail (this can happen under contention -- there's multiple consumer threads per sub-queue). If it did over-increment the head, a correction counter is incremented (making the queue eventually consistent), and if not, it goes ahead and increments another integer which gives it the actual final logical index. The increment of this final index always yields a valid index in the actual queue, regardless of what other threads are doing or have done; this works because the final index is only ever incremented when there's guaranteed to be at least one element to dequeue (which was checked when the first index was incremented)

* æ‰¹å¤„ç†æ€§èƒ½å¯¹äºblockè€Œè¨€æ˜¯ä¼˜è¶Šçš„
* o simplify the interface, if no token is provided by the user for a producer, [a lock-free hash table](#user-content-fn-1)[^1] is used (keyed to the current thread ID) to look up a thread-local producer queue
* æ‰¹å¤„ç†æ—¶çš„æ´»é”å¯èƒ½æ€§ï¼Ÿ

> Since tokens contain what amounts to thread-specific data, they should never be used from multiple threads simultaneously (although it's OK to transfer ownership of a token to another thread; in particular, this allows tokens to be used inside thread-pool tasks even if the thread running the task changes part-way through)

* All the producer queues link themselves together into a lock-free linked list.
*

## boost::lockfree::queue

* mpmc
* CASå¾ªç¯å…¥åˆ—ï¼Œåº•å±‚ç”¨é“¾è¡¨
* detail::select\_freelist\<node, node\_allocator, compile\_time\_sized, fixed\_sized, capacity>::type pool\_t;ä¼¼ä¹æ˜¯ä¸€ä¸ªå†…å­˜æ± æ¥ç®¡ç†æ–°èŠ‚ç‚¹çš„å†…å­˜åˆ†é…,å…·ä½“æ˜¯ä¸€ä¸ªå¯æ‰©å®¹çš„ç©ºé—²é“¾è¡¨
* èŠ‚ç‚¹çš„nextæ˜¯åŸå­å˜é‡ï¼Œé˜Ÿåˆ—çš„headå’Œtailä¸ºåŸå­å˜é‡

## boost::lockfree::spsp\_queue

* æ»¡æ»¡çš„æ¨¡æ¿ï¼Œå±‚æ¬¡äº¤é”™ï¼Œç”¨æ¨¡æ¿å…ƒæ¥é™ä½è€¦åˆåº¦ï¼Œæ‰©å±•æ€§è¾ƒé«˜ã€‚
* åŸºäºringbuffer,å…¶ä¸­å®šé•¿ä¸ä¸å®šé•¿å‡åŸºäºringbuffer\_baseï¼Œè€Œringbuffer\_baseä¸­ä¹Ÿç”¨äº†paddingè®©write\_index\_å’Œread\_index\_ä¸åœ¨åŒä¸€ç¼“å­˜è¡Œã€‚
* åŸºäºåŒä¸€å—å†…å­˜ä¸Šçš„write\_indexå’Œread\_index\_ç®¡ç†ï¼Œindexå‡æŒ‡çš„æ˜¯åœ¨æŒ‡å®šbufferä¸Šçš„å­—èŠ‚åç§»é‡ï¼Œå‡ä¸ºåŸå­å˜é‡ã€‚
*

    ```cpp
    Requirements:
     *  - T must have a default constructor
     *  - T must be copyable or movable
    ```

## DNedic/lockfree::spsc::Queue

* ä»…æ”¯æŒpushå’Œpop,åˆ©ç”¨å¾ªç¯ç¼“å†²åŒºå’Œreadã€writeçš„atomicå˜é‡å®ç°ã€‚

## DNedic/lockfree::mpmc::Queue

* ä»…æ”¯æŒpushå’Œpopï¼Œåˆ©ç”¨å¾ªç¯ç¼“å†²åŒºå®ç°ï¼Œä¸ºé¿å…ç«äº‰ï¼Œæ¯ä¸ªå…ƒç´ éƒ½è¢«slotå°è£…ï¼Œæ¯ä¸ªslotåŒ…å«push\_countå’Œpop\_countatomicå˜é‡ç”¨äºé˜²æ­¢ç«äº‰ï¼Œå¯¹rã€w\_countsé˜Ÿåˆ—çš„åŸå­å˜é‡è¿›è¡Œcaså¾ªç¯æ¥è·å¾—ç«äº‰æƒã€‚
* æ˜¾ç„¶O(size)ä¸ªatomicå˜é‡é™ä½äº†æ€§èƒ½é¢„æœŸ

## max0x7ba/atomic\_queue::AtomicQueue

### Design Principles

* Bare minimum of atomic instructions. Inlinable by default push and pop functions can hardly be any cheaper in terms of CPU instruction number / L1i cache pressure.
* Explicit contention/false-sharing avoidance for queue and its elements.
* Linear fixed size ring-buffer array. No heap memory allocations after a queue object has constructed. It doesn't get any more CPU L1d or TLB cache friendly than that.
* Value semantics. Meaning that the queues make a copy/move upon `push`/`pop`, no reference/pointer to elements in the queue can be obtained.

***

**ä»»åŠ¡æ”¹å˜ï¼Œæš‚åœè°ƒç ”ã€‚**

[^1]: what?
